Particle physics is based on particle detection by sophisticated experimental devices and their comparison to accurate simulations. High Energy Physics (HEP) requires very large amounts of computing resources to analyze simulations and data recorded by the detectors. I have deep knowledge of planning, developing, maintaining and operating distributed computing infrastructures providing access to several hundred-thousand computing cores and many hundred of petabytes of disk space. I am intimately familiar with scientific grid sites, academic and commercial clouds and the largest supercomputers at High Performance Computing centers in the U.S. and across the world. The infrastructures execute scientific software consisting of millions of lines of C++ and python code is needed to extract physics results. I am an expert in object oriented software development, statistical data analysis methods and Monte Carlo simulation techniques as well as various optimization and machine learning techniques.

These technical aspects of my work are closely connected to my physics research, as they enable the analysis of particle physics detector data and simulations as a basis to extract physics results.

### Computing Infrastructure

Since 2017, I am employing my vast knowledge of scientific software and computing in contributing to the worldwide community efforts to plan for the software and computing infrastucture for the High Luminosity LHC (HL-LHC). Starting in 2026, the HL-LHC will produce 10 times the amount of data of the current LHC running periods. The collisions and the corresponding simulations will be many times as complex as today. I was an integral part of the community planning process and my input was documented in the [Roadmap for HEP Software and Computing R&D for the 2020s](http://arxiv.org/abs/1712.06982). In addition, I was editor of the [HEP Software Foundation Community White Paper Working Group - Data Analysis and Interpretation](http://arxiv.org/abs/1804.03983).

My recent research interest in computing infrastructure is asking the question if analysis in HEP can be conducted more efficiently using tools developed and used by industry. Instead of employing the [ROOT](https://root.cern/) toolkit that was entirely developed and is maintained by the HEP community, I am exploring using toolkits like [Apache Spark](https://spark.apache.org/) or similar technologies. I created a research group spanning researchers from Fermilab, CERN and the Universities Princeton, Padova and Vanderbilt. The [CMS Big Data Project](https://cms-big-data.github.io/) also very closely works together with industry, latest in a project with [Intel](https://www.intel.com/) in the context of [CERN openlab](https://openlab.cern/).

I am currently the U.S. CMS Software and Computing Operations Program deputy manager overseeing a budget of $16M to enable analysis of U.S. collaborators of the CMS experiment. I am overseeing the operation of the U.S. CMS Tier-1 site at Fermilab and 7 U.S. Tier-2 sites at Caltech, Florida University, MIT, University of Nebraska-Lincoln, Purdue University, UC San Diego, Unversity of Wisconsin-Madison. The program also funds over 80 FTE of effort to administer the sites, maintain the computing infrastructure and conduct strategic R&D projects.

The CMS collaboration appointed me 2015 Focus Area Lead for Services and Infrastructure in the CMS Software and Computing project. I am coordinating the efforts of the worldwide submission infrastructure, innovative new ways of using resources at commercial clouds and supercomputing centers, and the development of computing insfrastructure services like data management and workflow management systems.

From September 2014 to September 2016, I was appointed Assistant Scientific Computing Division Head for Science Operations and Workflows in the Scientific Computing Division of Fermilab. I was responsible for the delivery of scientific computing services to all Fermi National Accelerator Laboratory experiments including High Energy Physics experiments (e.g. CMS), Neutrino Physics experiments (e.g. NOvA, Minerva), Intensity Frontier experiments (e.g. mu2e, Muon g-2) and Astroparticle Physics experiments (e.g. DES). As member of the senior management team, I developed strategic plans to evolve the infrastructure and operational procedures. For example, I developed a new storage strategy that simplifies the operation and usage of the more than 30 PB of disk space at Fermilab. I was also responsible for maintaining the computing strategy as part of the Laboratory Strategy Effort and reported to the laboratory directorate.

The CMS collaboration appointed me lead of the Data Operations Project in 2009. Using my deep involvement in analysis and my expertise in computing, I was responsible for the timely delivery of all data and MC samples for analysis, a significant contribution to the overall success of the experiment. In 2012 till 2014, CMS extended my responsibilities and appointed me to lead all of the CMS Computing Operations Project, adding the care of over 60 computing centers distributed all over the world and all central computing services of CMS. I was supervising the contributions of more than 60 scientists and engineers to the Computing Operations Project worldwide. The team was overseeing the readiness of all the computing facilities and monitor both central workflows and analysis and the transfers of data and MC samples between the sites. I was a L2 manager in the U.S. CMS Software & Computing Operations Program responsible for Computing Operations. In this capacity and also before, I reported regularly to the funding agencies and took part in reviews of DOE and NSF.

### Software

In the U.S. CMS Software and Computing project, I am also curently responsible for the Software and Support area. Under my guidance, the CMS software framework CMSSW is developed, as well as critical R&D is conduected in the areas of vectorized tracking software, machine learning and novel analysis facilities.

Since 2005 and before the start of the LHC data taking in 2010, I was deeply involved in getting the CMS software ready for data taking. I was the lead developer of an innovative tracking algorithm that was used during the commissioning of the CMS detector. I conducted the first software tutorials in CMS teaching the basics of analysis software and how to perform analysis on the GRID to the CMS community, using a user-friendly GRID analysis tool, of which I was one of the lead developers as well.

During my graduate student time at DESY from 2001 to 2005, I was one of the proponents and lead developers of a new object-oriented and ROOT-based event display. The upgrade of the ZEUS detector made it necessary to integrate the new and changed detector components in the event visualization solution of ZEUS. A client-server structure allowed physicists to display events without direct access to the event store. Online events could also be displayed worldwide with very small latency during data taking.
