include({{{{../cv/technical_experience_introduction.md}}}})

## Managed Networks

Distributed data-intensive computing depends on high-speed wide-area network connectivity to move data efficiently for processing workflows or end-user analysis. I actively invest in networking R\&D, participating in the [DOE ASCR ESnet](https://www.es.net) [requirements review](https://escholarship.org/uc/item/78j3c9v40) and leading efforts with my team on key networking topics. Of particular interest are dynamically managed network paths. The ESnet [SENSE/Rucio](https://indico.jlab.org/event/459/contributions/11306/) project focuses on enabling the data management system [Rucio](https://rucio.cern.ch) to dynamically create network paths between storage endpoints, thereby guaranteeing quality of service, improving predictability, and reducing contention in organized data flows.

## AI-Based Flow Characterization

In collaboration with ESnet researchers, I am working to characterize network flows at site borders by analyzing packet headers. We employ advanced AI techniques to distinguish between streaming, file transfers, and other flow types. This capability will enable deployment of managed network paths via SENSE without requiring instrumentation at the application layer.

## Enabling HPC Utilization

U.S. High Performance Computing centers, especially DOE leadership-class facilities at Argonne, Oak Ridge, and NERSC, offer exceptional computational capabilities that open new frontiers in scientific research. These centers are optimized for computationally intensive workflows, prioritizing performance per watt. Many sites now use GPUs to maximize performance while minimizing power consumption. For High Energy Physics (HEP), transitioning experimental software and infrastructure to leverage GPUs is a pivotal challenge, arguably as significant as the historic shift from Fortran to object-oriented C++. I consider this transition essential for future scientific success.

I actively contribute to both software and infrastructure efforts supporting this transition. The HEP-CCE project, which I conceptualized, advances portability libraries enabling developers to write algorithms once and compile or execute them seamlessly across diverse CPU and GPU architectures, thereby reducing software development overhead. Under my guidance, the U.S. CMS Software & Computing Operations Program supports R\&D on GPU-based tracking software and, through its R\&D initiative, on many additional algorithms undergoing GPU adaptation. Through the [Fermilab HEPCloud project](https://computing.fnal.gov/hep-cloud/), I am facilitating access to numerous HPC centers for CMS and the Fermilab community, contingent on allocation approvals by respective experiments. Combined with appropriate software, this approach aims to enable seamless future integration.

## End-User Analysis

Another focus of my research in computing infrastructure is exploring whether High Energy Physics analysis can be conducted more efficiently using tools developed and widely adopted in industry. Rather than relying exclusively on the [ROOT](https://root.cern/) toolkit, which was developed entirely within the HEP community, I am investigating industry-standard platforms like [Apache Spark](https://spark.apache.org/) and similar technologies. To this end, I initiated a collaborative research group including scientists from Fermilab, CERN, and the universities of Princeton, Padova, and Vanderbilt. The [CMS Big Data Project](https://cms-big-data.github.io/) worked closely with industry partners, most recently concluding a project with [Intel](https://www.intel.com/) in January 2019 under the auspices of [CERN openlab](https://openlab.cern/). To enable this collaboration, Fermilab joined CERN openlab, and I coordinated the DOE approval process with support from the Fermilab Office of Partnership and Technology Transfer and Legal Office.

I also managed a Laboratory Directed Research and Development (LDRD) project to develop innovative Big Data delivery technology for array-based analysis code: the [Striped Data Server for Scalable Parallel Data Analysis](https://doi.org/10.1088/1742-6596/1085/4/042035). Completed successfully in January 2019, this project produced a working prototype. These efforts have inspired numerous researchers and software experts, spawning projects such as the columnar data tools user front-end [COFFEA](https://github.com/CoffeaTeam) and analysis support funded through [IRIS-HEP](https://iris-hep.org/as.html).

These developments culminate in the creation of the analysis facility concept, built around the Python-based analysis ecosystem and graph scheduling tools like [Dask](https://www.dask.org). The analysis facility concept is now central to the U.S. CMS Software & Computing Operations Programâ€™s strategy for the HL-LHC.

I am not alone in advancing these technologies, especially junior scientists are drawn to them, as they provide transferable skills valuable both in academia and industry.
