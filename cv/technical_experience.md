include({{{{../cv/technical_experience_introduction.md}}}})

## Managed networks

Distributed data-intensive computing relies on very fast wide-area network connectivity to move data to where it is needed, either for processing workflows or end-user analysis. I am investing in networking R&D both by being part of the [DOE ASCR's ESnet](https://www.es.net) [requirements review](https://escholarship.org/uc/item/78j3c9v40) and contributing actively with my team to networking R&D topics. Especially of interest are dynamically managed network paths. The ESnet [SENSE/Rucio](https://indico.jlab.org/event/459/contributions/11306/) project works on a solution that the data management system [Rucio](https://rucio.cern.ch) can dynamically create network paths between storage endpoints and through this guarantee quality of service and improve predictability and reduce contention of organized data flows. 

## AI-based flow characterization

Together with ESnet researchers, I am working on characterizing network flows at the site border using packet headers. We are deploying sophisticated AI techniques to learn the patterns of streaming vs. file transfers vs. other flow types. This will enable us to deploy managed network paths using SENSE without having to instrument the application layer.

## Enabling HPC utilization

High Performance Computing centers in the U.S., especially the DOE leadership class facilities at Argonne, Oak Ridge and NERSC, provide extraordinary computing capabilities and could open new avenues for scientific research. These facilities are designed for the largest computationally intensive workflows and optimize on performance per Watt. Currently many sites are using GPUs to achieve the best performance while keeping the power consumption low. HEP experimental software and infrastructure need to transition to be able to use GPUs. This transition is as big or even bigger than the switch from Fortran to object-oriented C++. I consider this transition crucial for being successful in the science harvest in the future. 

I invest both in the software and infrastructure to enable this transition. The HEP-CCE project I conceptualized paves the way for portability libraries that allow to develop an algorithm once and then compile/execute it on many different GPUs and CPUs, reducing the overhead for software development. Under my guidance, the U.S. CMS Software & Computing Operations Program supports R&D on tracking software on GPUs and through the R&D initiative on many more algorithms and their GPU transition. Through the [Fermilab HEPCloud project](https://computing.fnal.gov/hep-cloud/) I am making many different HPC centers accessible to CMS and the Fermilab community, if allocations by the different experiments have been attained. Together with the right software, this should provide for a seamless integration for the future.

## End-User Analysis

My other research interest in computing infrastructure is asking the question if analysis in HEP can be conducted more efficiently using tools developed and used by industry. Instead of employing exclusively the [ROOT](https://root.cern/) toolkit that was entirely developed by the HEP community, I am exploring using toolkits used in industry like [Apache Spark](https://spark.apache.org/) or similar technologies. As a first step, I started a research group spanning researchers from Fermilab, CERN and the Universities Princeton, Padova and Vanderbilt. The [CMS Big Data Project](https://cms-big-data.github.io/) was very closely working together with industry, latest in a project with [Intel](https://www.intel.com/) concluded in January 2019 in the context of [CERN openlab](https://openlab.cern/). To realize this project, Fermilab joined CERN openlab and I organized the DOE approval process with the help of the Fermilab Office of Partnership and Technology Transfer and the Fermilab Legal Office. I also managed a Laboratory Directed Research and Development project (LDRD) to develop innovative technology for Big Data delivery to array-based analysis code, the [Striped Data Server for Scalable Parallel Data Analysis](https://doi.org/10.1088/1742-6596/1085/4/042035). The project concluded successfully in January 2019 and produced a prototype. These projects inspired many researchers and software experts and led to various projects like a user front-end to columnar data tools [COFFEA](https://github.com/CoffeaTeam) and supporting analysis activities funded through [IRIS-HEP](https://iris-hep.org/as.html). 

These developments are culminating into the creation of the analysis facility concept based on the pythonic analysis ecosystem and graph scheduling tools like [Dask](https://www.dask.org). The analysis facility concept is now at the heart of the U.S. CMS Software & Computing Operations Program strategy for HL-LHC. 

I am not alone in pursuing these technologies. Especially junior scientists are drawn to these technologies because they learn transferrable skills that can be used in industry.