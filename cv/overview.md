I am a particle physicist and conduct **leading edge research** for New Physics Beyond the Standard Model of Particle Physics. This goes hand in hand with further consolidating the validity of the Standard Model in the absence of new physics signals.

I have multiple years of experience in analyzing high-energy collisions at different particle colliders using a multitude of different techniques. I have [**published many papers in leading journals**](https://github.com/gutsche/ForThePublic/raw/master/publication_list/complete_publication_list.pdf) and am currently a member of the CMS collaboration at the Large Hadron Collider (LHC) at [CERN](https://home.cern/). In my recent studies at the LHC, I have lead searches for evidence of physics beyond the Standard Model using top quarks, and contributed to searches for Supersymmetry and Dark Matter. One of my most noticeable publications is the [**Observation of the Higgs Boson in 2012**](https://doi.org/10.1016/j.physletb.2012.08.021).

I am a **leader in scientific computing** and have acquired deep knowledge and expertise in scientific software and computing. Particle physics is based on particle detection by sophisticated experimental devices and their comparison to accurate simulations. Scientific software consists of millions of lines of C++ and python code and is needed to extract these physics results. I am an expert in object oriented software development, statistical data analysis methods and Monte Carlo simulation techniques as well as various optimization and machine learning techniques. High Energy Physics (HEP) requires very large amounts of computing resources to analyze simulations and data recorded by the detectors. I have deep experience in planning, developing, and operating distributed computing infrastructures that provide access to several hundred-thousand computing cores and hundreds of petabytes of disk space. I am intimately familiar with scientific grid sites, academic and commercial clouds and the largest U.S. supercomputers . Recently I was part of a worldwide community planning process for the software and computing infrastructure of the High Luminosity LHC (HL-LHC, 2016). The upgraded machine and detectors will require 20 times as much computing resources as today. The goal of the community planning exercise is to reduce this increase significantly through disruptive changes to software and computing. Examples are new analysis paradigms using industry technologies like Apache Spark, and vectorized and SIMD programming technologies and machine learning approaches to exploit new hardware architectures like accelerators and GPUs. I contributed to the [overview white paper of the community](http://arxiv.org/abs/1712.06982) and was editor of the topical white paper about the future of [data analysis in High Energy Physics](http://arxiv.org/abs/1804.03983). This follows my recent interest to use idustry technologies for petabyte scale analysis and I started the [CMS Big Data Project](https://cms-big-data.github.io/). One component is the collaboration with Intel on scaling of analysis facilities to Petabytes. Fermilab recently joined [CERN openlab](https://openlab.cern/) to make this collaboration a possibility.

I held many management positions at the Fermi National Accelerator Laboratory and within the international CMS collaboration, supervising up to 100 individuals across many time zones. In September 2016, I was appointed **U.S.CMS Software and Computing Operations Deputy Program manager, overseeing a budget of $16M** to enable analysis of LHC particle collisions in the U.S. for the 2500 physicist strong CMS collaboration.
